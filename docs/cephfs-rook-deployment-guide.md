# CephFS + Rook + CSI å¿«é€Ÿéƒ¨ç½²æŒ‡å—

## æ¦‚è¿°

æœ¬æŒ‡å—å¸®åŠ©æ‚¨åœ¨ç§æœ‰IDCçš„K8sç¯å¢ƒä¸­å¿«é€Ÿéƒ¨ç½²CephFS + Rook + CSIï¼Œå®ç°é«˜æ€§èƒ½å…±äº«å­˜å‚¨å’Œå®¡è®¡åŠŸèƒ½ã€‚

## å‰ç½®è¦æ±‚

- Kubernetesé›†ç¾¤ï¼ˆ1.19+ï¼‰
- è‡³å°‘3ä¸ªèŠ‚ç‚¹ï¼ˆç”¨äºCephé›†ç¾¤ï¼‰
- æ¯ä¸ªèŠ‚ç‚¹è‡³å°‘50GBå¯ç”¨ç£ç›˜ç©ºé—´
- ç½‘ç»œè¿é€šæ€§è‰¯å¥½

## å¿«é€Ÿå¼€å§‹ï¼ˆ5æ­¥éƒ¨ç½²ï¼‰

### æ­¥éª¤1ï¼šå®‰è£…Rook Operatorï¼ˆ5åˆ†é’Ÿï¼‰

```bash
# åˆ›å»ºRookå‘½åç©ºé—´
kubectl create namespace rook-ceph

# å®‰è£…Rook Operator
kubectl apply -f https://raw.githubusercontent.com/rook/rook/release-1.12/cluster/examples/kubernetes/ceph/common.yaml
kubectl apply -f https://raw.githubusercontent.com/rook/rook/release-1.12/cluster/examples/kubernetes/ceph/operator.yaml

# ç­‰å¾…Operatorå°±ç»ª
kubectl wait --for=condition=ready pod -l app=rook-ceph-operator -n rook-ceph --timeout=300s
```

### æ­¥éª¤2ï¼šåˆ›å»ºCephé›†ç¾¤ï¼ˆ10åˆ†é’Ÿï¼‰

åˆ›å»º `ceph-cluster.yaml`ï¼š

```yaml
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    image: quay.io/ceph/ceph:v18.2.0
  dataDirHostPath: /var/lib/rook
  mon:
    count: 3
    allowMultiplePerNode: false
  storage:
    useAllNodes: true
    useAllDevices: true
    config:
      databaseSizeMB: "1024"
      journalSizeMB: "1024"
  mgr:
    count: 1
  dashboard:
    enabled: true
```

éƒ¨ç½²ï¼š

```bash
kubectl apply -f ceph-cluster.yaml

# ç­‰å¾…é›†ç¾¤å°±ç»ªï¼ˆçº¦5-10åˆ†é’Ÿï¼‰
kubectl wait --for=condition=ready cephcluster rook-ceph -n rook-ceph --timeout=600s
```

### æ­¥éª¤3ï¼šåˆ›å»ºCephFSæ–‡ä»¶ç³»ç»Ÿï¼ˆ5åˆ†é’Ÿï¼‰

åˆ›å»º `cephfs.yaml`ï¼š

```yaml
apiVersion: ceph.rook.io/v1
kind: CephFilesystem
metadata:
  name: myfs
  namespace: rook-ceph
spec:
  metadataPool:
    replicated:
      size: 3
  dataPools:
    - replicated:
        size: 3
  metadataServer:
    activeCount: 1
    activeStandby: true
```

éƒ¨ç½²ï¼š

```bash
kubectl apply -f cephfs.yaml

# ç­‰å¾…MDSå°±ç»ª
kubectl wait --for=condition=ready cephfilesystem myfs -n rook-ceph --timeout=300s
```

### æ­¥éª¤4ï¼šé…ç½®CSI StorageClassï¼ˆ5åˆ†é’Ÿï¼‰

åˆ›å»º `storageclass.yaml`ï¼š

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-cephfs
provisioner: rook-ceph.cephfs.csi.ceph.com
parameters:
  clusterID: rook-ceph
  fsName: myfs
  pool: myfs-data0
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
allowVolumeExpansion: true
reclaimPolicy: Retain
```

éƒ¨ç½²ï¼š

```bash
kubectl apply -f storageclass.yaml

# è®¾ç½®ä¸ºé»˜è®¤StorageClassï¼ˆå¯é€‰ï¼‰
kubectl patch storageclass rook-cephfs -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
```

### æ­¥éª¤5ï¼šæµ‹è¯•PVCåˆ›å»ºå’Œä½¿ç”¨ï¼ˆ5åˆ†é’Ÿï¼‰

åˆ›å»ºæµ‹è¯•PVC `test-pvc.yaml`ï¼š

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: cephfs-pvc
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: rook-cephfs
  resources:
    requests:
      storage: 10Gi
```

åˆ›å»ºæµ‹è¯•Pod `test-pod.yaml`ï¼š

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: cephfs-test-pod
spec:
  containers:
    - name: test
      image: busybox
      command: ["sleep", "3600"]
      volumeMounts:
        - name: cephfs-vol
          mountPath: /mnt/cephfs
  volumes:
    - name: cephfs-vol
      persistentVolumeClaim:
        claimName: cephfs-pvc
```

æµ‹è¯•ï¼š

```bash
# åˆ›å»ºPVC
kubectl apply -f test-pvc.yaml

# åˆ›å»ºæµ‹è¯•Pod
kubectl apply -f test-pod.yaml

# éªŒè¯æŒ‚è½½
kubectl exec -it cephfs-test-pod -- ls -la /mnt/cephfs

# å†™å…¥æµ‹è¯•
kubectl exec -it cephfs-test-pod -- sh -c "echo 'Hello CephFS' > /mnt/cephfs/test.txt"

# è¯»å–æµ‹è¯•
kubectl exec -it cephfs-test-pod -- cat /mnt/cephfs/test.txt
```

## å®¡è®¡åŠŸèƒ½é…ç½®

### 1. è®¿é—®Ceph Dashboard

```bash
# è·å–Dashboardè®¿é—®ä¿¡æ¯
kubectl get svc -n rook-ceph rook-ceph-mgr-dashboard

# è·å–adminå¯†ç 
kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath="{['data']['password']}" | base64 --decode && echo

# ç«¯å£è½¬å‘ï¼ˆæœ¬åœ°è®¿é—®ï¼‰
kubectl port-forward -n rook-ceph svc/rook-ceph-mgr-dashboard 8443:8443
```

è®¿é—®ï¼šhttps://localhost:8443

### 2. å¯ç”¨å®¡è®¡æ—¥å¿—

åœ¨Ceph Dashboardä¸­ï¼š
1. è¿›å…¥ **Configuration** â†’ **MDS**
2. å¯ç”¨ `mds_audit_logging = true`
3. é…ç½®æ—¥å¿—çº§åˆ«å’Œè¾“å‡ºä½ç½®

### 3. æŸ¥çœ‹å®¡è®¡æ—¥å¿—

```bash
# æŸ¥çœ‹MDS Podæ—¥å¿—
kubectl logs -n rook-ceph -l app=rook-ceph-mds --tail=100

# å¯¼å‡ºå®¡è®¡æ—¥å¿—åˆ°æ–‡ä»¶
kubectl logs -n rook-ceph -l app=rook-ceph-mds > ceph-audit.log
```

### 4. é›†æˆåˆ°ELK/EFKï¼ˆå¯é€‰ï¼‰

åˆ›å»º `audit-log-forwarder.yaml`ï¼š

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: ceph-audit-config
  namespace: rook-ceph
data:
  fluent.conf: |
    <source>
      @type tail
      path /var/log/ceph/audit.log
      pos_file /var/log/ceph/audit.log.pos
      tag ceph.audit
      <parse>
        @type json
      </parse>
    </source>
    <match ceph.audit>
      @type elasticsearch
      host elasticsearch.logging.svc.cluster.local
      port 9200
      index_name ceph-audit
      type_name _doc
    </match>
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. å­˜å‚¨åç«¯ä¼˜åŒ–

```yaml
# åœ¨ceph-cluster.yamlä¸­æ·»åŠ 
spec:
  storage:
    config:
      # ä½¿ç”¨SSDæ—¶ä¼˜åŒ–
      osd_pool_default_pg_num: "128"
      osd_pool_default_pgp_num: "128"
      # å¯ç”¨å‹ç¼©ï¼ˆå¦‚æœCPUå……è¶³ï¼‰
      bluestore_compression_algorithm: snappy
```

### 2. ç½‘ç»œä¼˜åŒ–

```yaml
# é…ç½®ä¸“ç”¨å­˜å‚¨ç½‘ç»œï¼ˆå¦‚æœæœ‰å¤šç½‘å¡ï¼‰
spec:
  network:
    provider: host
    selectors:
      public: "public-network"
      cluster: "cluster-network"
```

### 3. MDSæ€§èƒ½è°ƒä¼˜

```yaml
# åœ¨CephFilesystemä¸­æ·»åŠ 
spec:
  metadataServer:
    activeCount: 2  # å¢åŠ MDSæ•°é‡
    resources:
      limits:
        cpu: "2"
        memory: "4Gi"
      requests:
        cpu: "1"
        memory: "2Gi"
```

## åœ¨Volcanoè®­ç»ƒä»»åŠ¡ä¸­ä½¿ç”¨

æ›´æ–° `training-job.yaml`ï¼š

```yaml
apiVersion: batch.volcano.sh/v1alpha1
kind: Job
metadata:
  name: training-job-with-cephfs
spec:
  schedulerName: volcano
  queue: training-queue
  minAvailable: 1
  tasks:
    - replicas: 1
      name: trainer
      template:
        spec:
          containers:
            - image: nvidia/cuda:11.8.0-base-ubuntu22.04
              name: trainer
              volumeMounts:
                - name: training-data
                  mountPath: /data
              command:
                - /bin/bash
                - -c
                - |
                  # è®­ç»ƒä»£ç è®¿é—® /data ç›®å½•
                  python train.py --data-dir=/data
          volumes:
            - name: training-data
              persistentVolumeClaim:
                claimName: training-data-pvc
          restartPolicy: Never
```

åˆ›å»ºè®­ç»ƒæ•°æ®PVCï¼š

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: training-data-pvc
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: rook-cephfs
  resources:
    requests:
      storage: 500Gi
```

## æ•…éšœæ’æŸ¥

### é—®é¢˜1ï¼šPVCä¸€ç›´Pending

```bash
# æ£€æŸ¥CSIé©±åŠ¨
kubectl get pods -n rook-ceph | grep csi

# æŸ¥çœ‹PVCäº‹ä»¶
kubectl describe pvc cephfs-pvc

# æ£€æŸ¥StorageClass
kubectl get storageclass rook-cephfs -o yaml
```

### é—®é¢˜2ï¼šPodæ— æ³•æŒ‚è½½

```bash
# æ£€æŸ¥CSI Nodeæ’ä»¶
kubectl logs -n rook-ceph -l app=csi-cephfsplugin --tail=100

# æ£€æŸ¥èŠ‚ç‚¹ä¸Šçš„æŒ‚è½½
kubectl debug node/<node-name> -it --image=busybox
# åœ¨debugå®¹å™¨ä¸­ï¼šmount | grep ceph
```

### é—®é¢˜3ï¼šæ€§èƒ½ä¸ä½³

```bash
# æ£€æŸ¥Cephé›†ç¾¤çŠ¶æ€
kubectl exec -n rook-ceph -it rook-ceph-tools -- ceph -s

# æ£€æŸ¥OSDçŠ¶æ€
kubectl exec -n rook-ceph -it rook-ceph-tools -- ceph osd df

# æ£€æŸ¥ç½‘ç»œå»¶è¿Ÿ
kubectl exec -n rook-ceph -it rook-ceph-tools -- ceph osd perf
```

## ç›‘æ§å’Œå‘Šè­¦

### 1. å®‰è£…Prometheusç›‘æ§

Rookè‡ªåŠ¨æš´éœ²PrometheusæŒ‡æ ‡ï¼š

```bash
# æŸ¥çœ‹ServiceMonitor
kubectl get servicemonitor -n rook-ceph

# é…ç½®PrometheusæŠ“å–ï¼ˆå¦‚æœä½¿ç”¨Prometheus Operatorï¼‰
# Rookä¼šè‡ªåŠ¨åˆ›å»ºServiceMonitor
```

### 2. å…³é”®æŒ‡æ ‡

- `ceph_cluster_total_bytes`: é›†ç¾¤æ€»å®¹é‡
- `ceph_cluster_total_used_bytes`: å·²ä½¿ç”¨å®¹é‡
- `ceph_mds_client_requests`: MDSè¯·æ±‚æ•°
- `ceph_osd_op_r_latency`: è¯»æ“ä½œå»¶è¿Ÿ
- `ceph_osd_op_w_latency`: å†™æ“ä½œå»¶è¿Ÿ

## å‚è€ƒèµ„æº

- **Cephå®˜æ–¹æ–‡æ¡£**: https://docs.ceph.com/
- **Rookå®˜æ–¹æ–‡æ¡£**: https://rook.io/docs/rook/latest/
- **ceph-csi GitHub**: https://github.com/ceph/ceph-csi
- **Ceph Dashboard**: https://docs.ceph.com/en/latest/mgr/dashboard/

## ä¸‹ä¸€æ­¥

1. âœ… å®ŒæˆPOCéƒ¨ç½²
2. ğŸ“Š è¿›è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
3. ğŸ” éªŒè¯å®¡è®¡åŠŸèƒ½
4. ğŸš€ å°è§„æ¨¡è¯•ç‚¹
5. ğŸ“ˆ ç”Ÿäº§éƒ¨ç½²

